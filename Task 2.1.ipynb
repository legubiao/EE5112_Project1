{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2dfcac-3e77-4386-a09f-a6ca8f77350c",
   "metadata": {},
   "source": [
    "# Task 2: Understand body language by gesture recognition with convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b2526-0909-476e-82ca-f688785da04d",
   "metadata": {},
   "source": [
    "## 1. Do literature search on Convolution Neural Network. Learn how to build a convolutional layer in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc76db6-4a78-42d2-b6e7-8835a48df88c",
   "metadata": {},
   "source": [
    "## 2. Referring to the guide in Task 1, build your own network for gesture classification using convolutional layers. Please see the references 4 in the manual to learn how to build convolutional layers in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f296982f31bb236",
   "metadata": {},
   "source": [
    "## 3. Analyse and comment on the performance of the model. Make a comparison between the fully connected based and convolutional based models and comment on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a84baf415f42dac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T08:38:24.298391Z",
     "start_time": "2023-09-14T08:38:23.441571Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import torch.utils.data as utils_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9af536fdcd128",
   "metadata": {},
   "source": [
    "# 1) data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0e9ee29471dfa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T08:38:24.301662Z",
     "start_time": "2023-09-14T08:38:24.299894Z"
    }
   },
   "outputs": [],
   "source": [
    "def processSkinImage(filePath, resize_HW=48):\n",
    "    # step 1\n",
    "    # read the image\n",
    "    original = cv2.imread(filename=filePath)\n",
    "\n",
    "    # step 2\n",
    "    # resize the image to\n",
    "    image_resized = cv2.resize(original, (resize_HW, resize_HW))\n",
    "\n",
    "    # step 3\n",
    "    # convert the image from rgb to YCbCr\n",
    "    image_ycbcr = cv2.cvtColor(image_resized, cv2.COLOR_BGR2YCR_CB)\n",
    "\n",
    "    # step 4\n",
    "    # get the central color of the image\n",
    "    # expected the hand to be in the central of the image\n",
    "    Cb_center_color = image_ycbcr[int(resize_HW/2), int(resize_HW/2), 1]\n",
    "    Cr_center_color = image_ycbcr[int(resize_HW/2), int(resize_HW/2), 2]\n",
    "    # set the range\n",
    "    Cb_Difference = 15\n",
    "    Cr_Difference = 10\n",
    "\n",
    "    # step 5\n",
    "    # detect skin pixels\n",
    "    Cb = image_ycbcr[:, :, 1]\n",
    "    Cr = image_ycbcr[:, :, 2]\n",
    "    index = np.where((Cb >= Cb_center_color-Cb_Difference) & (Cb <= Cb_center_color+Cb_Difference)\n",
    "                     & (Cr >= Cr_center_color-Cr_Difference) & (Cr <= Cr_center_color+Cr_Difference))\n",
    "\n",
    "    # Mark detected pixels and output\n",
    "    image_output = np.zeros((resize_HW, resize_HW))\n",
    "    image_output[index] = 255\n",
    "\n",
    "    # show image\n",
    "    # cv2.imshow(\"\", image_output)\n",
    "    # cv2.waitKey(0)\n",
    "    \n",
    "    return image_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f4305de2426b31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T08:38:24.410452Z",
     "start_time": "2023-09-14T08:38:24.303249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/images/right/right (4).jpg\n",
      "./dataset/images/right/right (13).jpg\n",
      "./dataset/images/right/right (8).jpg\n",
      "./dataset/images/right/right (24).jpg\n",
      "./dataset/images/right/right (9).jpg\n",
      "./dataset/images/right/right (12).jpg\n",
      "./dataset/images/right/right (5).jpg\n",
      "./dataset/images/right/right (23).jpg\n",
      "./dataset/images/right/right (19).jpg\n",
      "./dataset/images/right/right (2).jpg\n",
      "./dataset/images/right/right (15).jpg\n",
      "./dataset/images/right/right (14).jpg\n",
      "./dataset/images/right/right (3).jpg\n",
      "./dataset/images/right/right (18).jpg\n",
      "./dataset/images/right/right (22).jpg\n",
      "./dataset/images/right/right (21).jpg\n",
      "./dataset/images/right/right (17).jpg\n",
      "./dataset/images/right/right (1).jpg\n",
      "./dataset/images/right/right (16).jpg\n",
      "./dataset/images/right/right (20).jpg\n",
      "./dataset/images/right/right (11).jpg\n",
      "./dataset/images/right/right (6).jpg\n",
      "./dataset/images/right/right (7).jpg\n",
      "./dataset/images/right/right (10).jpg\n",
      "./dataset/images/left/left (8).jpg\n",
      "./dataset/images/left/left (25).jpg\n",
      "./dataset/images/left/left (13).jpg\n",
      "./dataset/images/left/left (4).jpg\n",
      "./dataset/images/left/left (5).jpg\n",
      "./dataset/images/left/left (12).jpg\n",
      "./dataset/images/left/left (24).jpg\n",
      "./dataset/images/left/left (9).jpg\n",
      "./dataset/images/left/left (15).jpg\n",
      "./dataset/images/left/left (2).jpg\n",
      "./dataset/images/left/left (19).jpg\n",
      "./dataset/images/left/left (23).jpg\n",
      "./dataset/images/left/left (22).jpg\n",
      "./dataset/images/left/left (18).jpg\n",
      "./dataset/images/left/left (3).jpg\n",
      "./dataset/images/left/left (14).jpg\n",
      "./dataset/images/left/left (17).jpg\n",
      "./dataset/images/left/left (21).jpg\n",
      "./dataset/images/left/left (20).jpg\n",
      "./dataset/images/left/left (16).jpg\n",
      "./dataset/images/left/left (1).jpg\n",
      "./dataset/images/left/left (27).jpg\n",
      "./dataset/images/left/left (6).jpg\n",
      "./dataset/images/left/left (11).jpg\n",
      "./dataset/images/left/left (10).jpg\n",
      "./dataset/images/left/left (7).jpg\n",
      "./dataset/images/left/left (26).jpg\n",
      "./dataset/images/peace/peace (11).jpg\n",
      "./dataset/images/peace/peace (10).jpg\n",
      "./dataset/images/peace/peace (1).jpg\n",
      "./dataset/images/peace/peace (6).jpg\n",
      "./dataset/images/peace/peace (7).jpg\n",
      "./dataset/images/peace/peace (4).jpg\n",
      "./dataset/images/peace/peace (8).jpg\n",
      "./dataset/images/peace/peace (9).jpg\n",
      "./dataset/images/peace/peace (5).jpg\n",
      "./dataset/images/peace/peace (14).jpg\n",
      "./dataset/images/peace/peace (13).jpg\n",
      "./dataset/images/peace/peace (2).jpg\n",
      "./dataset/images/peace/peace (3).jpg\n",
      "./dataset/images/peace/peace (12).jpg\n",
      "./dataset/images/palm/palm (4).jpg\n",
      "./dataset/images/palm/palm (8).jpg\n",
      "./dataset/images/palm/palm (11).jpg\n",
      "./dataset/images/palm/palm (10).jpg\n",
      "./dataset/images/palm/palm (9).jpg\n",
      "./dataset/images/palm/palm (5).jpg\n",
      "./dataset/images/palm/palm (2).jpg\n",
      "./dataset/images/palm/palm (3).jpg\n",
      "./dataset/images/palm/palm (1).jpg\n",
      "./dataset/images/palm/palm (6).jpg\n",
      "./dataset/images/palm/palm (13).jpg\n",
      "./dataset/images/palm/palm (12).jpg\n",
      "./dataset/images/palm/palm (7).jpg\n"
     ]
    }
   ],
   "source": [
    "path = './dataset/images'\n",
    "path_processed = './dataset_processed/images'\n",
    "\n",
    "# -------------------images processing--------------\n",
    "for mainDir, subDir, fileList in os.walk(path):\n",
    "    for file in fileList:\n",
    "        currentPath = os.path.join(mainDir, file)\n",
    "        print(currentPath)\n",
    "        processedImage = processSkinImage(currentPath)\n",
    "\n",
    "        new_mainDir = path_processed + mainDir.split(path)[-1]\n",
    "        if not os.path.exists(new_mainDir):\n",
    "            os.makedirs(new_mainDir)\n",
    "        cv2.imwrite(os.path.join(new_mainDir, file), processedImage)\n",
    "\n",
    "# -----------------label generation----------------\n",
    "label_path = './dataset_processed/labels'\n",
    "if not os.path.exists(label_path):\n",
    "    os.makedirs(label_path)\n",
    "\n",
    "files = os.listdir(path)\n",
    "for i, file in enumerate(files):\n",
    "    if file == '.DS_Store':\n",
    "        continue\n",
    "    subclass_label_path = os.path.join(label_path, file+'.txt')\n",
    "    with open(subclass_label_path, 'w') as f:\n",
    "        f.write('#label\\n')\n",
    "    for _ in range(len(os.listdir(os.path.join(path_processed, file)))):\n",
    "        with open(subclass_label_path, 'a') as f:\n",
    "            f.write('{:d}\\n'.format(i))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b8df8b979adf6",
   "metadata": {},
   "source": [
    "# 2) load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2e1dc4de31ffe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T08:38:24.414238Z",
     "start_time": "2023-09-14T08:38:24.408492Z"
    }
   },
   "outputs": [],
   "source": [
    "Image = []\n",
    "path_images = './dataset_processed/images'\n",
    "for mainDir, subDir, fileList in os.walk(path_images):\n",
    "    for file in fileList:\n",
    "        currentPath = os.path.join(mainDir, file)\n",
    "        Image.append(cv2.imread(currentPath)[:, :, 0])\n",
    "Image = np.array(Image)\n",
    "dataset_size, H, W = Image.shape\n",
    "\n",
    "Label = []\n",
    "path_labels = './dataset_processed/labels'\n",
    "for file in os.listdir(path_labels):\n",
    "    Label.append(np.loadtxt(os.path.join(path_labels, file)))\n",
    "Label = np.array(list(itertools.chain.from_iterable(Label)))\n",
    "num_classes = int(np.max(Label))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe643214b9ed582",
   "metadata": {},
   "source": [
    "# 3) Build Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbbb34f4bf35144",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T08:38:24.676763Z",
     "start_time": "2023-09-14T08:38:24.674542Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1) # 卷积层1，输入通道数为3，输出通道数为16，卷积核大小为3x3，填充为1\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1) # 卷积层2，输入通道数为16，输出通道数为32，卷积核大小为3x3，填充为1\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1) # 卷积层3，输入通道数为32，输出通道数为64，卷积核大小为3x3，填充为1\n",
    "        self.pool = nn.MaxPool2d(2, 2) # 池化层，池化核大小为2x2，步长为2\n",
    "        self.fc1 = nn.Linear(64 * 48 // 8 * 48 // 8, 256) # 全连接层1，输入特征数为64 * w // 8 * h // 8，输出特征数为256\n",
    "        self.fc2 = nn.Linear(256, num_classes) # 全连接层2，输入特征数为256，输出特征数为num_classes\n",
    "        self.dropout = nn.Dropout(0.2) # 随机失活层，失活概率为0.2\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=x.unsqueeze(1)\n",
    "        x = self.pool(torch.relu(self.conv1(x))) # 卷积层1后接激活函数和池化层\n",
    "        x = self.pool(torch.relu(self.conv2(x))) # 卷积层2后接激活函数和池化层\n",
    "        x = self.pool(torch.relu(self.conv3(x))) # 卷积层3后接激活函数和池化层\n",
    "        x = x.view(-1, 64 * 48 // 8 * 48 // 8) # 将张量展平为一维向量\n",
    "        x = torch.relu(self.fc1(x)) # 全连接层1后接激活函数\n",
    "        x = self.dropout(x) # 随机失活层\n",
    "        x = self.fc2(x) # 全连接层2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faa0c511866683ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T08:38:24.864081Z",
     "start_time": "2023-09-14T08:38:24.857361Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = CNNModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595944852873cdc4",
   "metadata": {},
   "source": [
    "# 4) Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e5fdf1ef9d4e1aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T08:38:25.440656Z",
     "start_time": "2023-09-14T08:38:25.433944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dataset = utils_data.TensorDataset(torch.Tensor(Image), torch.LongTensor(Label))\n",
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_set, test_set = utils_data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = utils_data.DataLoader(dataset=train_set, batch_size=8, shuffle=True)\n",
    "test_loader = utils_data.DataLoader(dataset=test_set, batch_size=8, shuffle=True)\n",
    "print('Data is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44d11a7555b753db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T08:38:27.951562Z",
     "start_time": "2023-09-14T08:38:25.901654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\ttrain loss=15.518861\ttrain accuracy=0.403\ttest accuracy=0.812\n",
      "epoch=1\ttrain loss=0.731783\ttrain accuracy=0.694\ttest accuracy=0.688\n",
      "epoch=2\ttrain loss=0.396401\ttrain accuracy=0.887\ttest accuracy=0.750\n",
      "epoch=3\ttrain loss=0.275968\ttrain accuracy=0.887\ttest accuracy=0.688\n",
      "epoch=4\ttrain loss=0.187599\ttrain accuracy=0.935\ttest accuracy=0.750\n",
      "epoch=5\ttrain loss=0.066338\ttrain accuracy=0.984\ttest accuracy=0.812\n",
      "epoch=6\ttrain loss=0.068913\ttrain accuracy=0.968\ttest accuracy=0.812\n",
      "epoch=7\ttrain loss=0.032775\ttrain accuracy=1.000\ttest accuracy=0.750\n",
      "epoch=8\ttrain loss=0.032821\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=9\ttrain loss=0.015659\ttrain accuracy=1.000\ttest accuracy=0.750\n",
      "epoch=10\ttrain loss=0.003571\ttrain accuracy=1.000\ttest accuracy=0.750\n",
      "epoch=11\ttrain loss=0.007298\ttrain accuracy=1.000\ttest accuracy=0.750\n",
      "epoch=12\ttrain loss=0.007399\ttrain accuracy=1.000\ttest accuracy=0.688\n",
      "epoch=13\ttrain loss=0.054677\ttrain accuracy=0.984\ttest accuracy=0.750\n",
      "epoch=14\ttrain loss=0.060355\ttrain accuracy=0.984\ttest accuracy=0.750\n",
      "epoch=15\ttrain loss=0.074520\ttrain accuracy=0.984\ttest accuracy=0.875\n",
      "epoch=16\ttrain loss=0.018254\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=17\ttrain loss=0.019546\ttrain accuracy=1.000\ttest accuracy=0.562\n",
      "epoch=18\ttrain loss=0.019077\ttrain accuracy=0.984\ttest accuracy=0.812\n",
      "epoch=19\ttrain loss=0.007493\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=20\ttrain loss=0.004462\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=21\ttrain loss=0.002865\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=22\ttrain loss=0.001367\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=23\ttrain loss=0.001156\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=24\ttrain loss=0.000907\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=25\ttrain loss=0.001547\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=26\ttrain loss=0.000821\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=27\ttrain loss=0.000759\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=28\ttrain loss=0.000533\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=29\ttrain loss=0.000556\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=30\ttrain loss=0.000415\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=31\ttrain loss=0.000569\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=32\ttrain loss=0.000349\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=33\ttrain loss=0.000375\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=34\ttrain loss=0.000342\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=35\ttrain loss=0.000292\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=36\ttrain loss=0.000163\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=37\ttrain loss=0.000304\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=38\ttrain loss=0.000256\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=39\ttrain loss=0.000452\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=40\ttrain loss=0.000119\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=41\ttrain loss=0.000270\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=42\ttrain loss=0.000098\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=43\ttrain loss=0.000225\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=44\ttrain loss=0.000135\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=45\ttrain loss=0.000200\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=46\ttrain loss=0.000057\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=47\ttrain loss=0.000119\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=48\ttrain loss=0.000296\ttrain accuracy=1.000\ttest accuracy=0.812\n",
      "epoch=49\ttrain loss=0.000083\ttrain accuracy=1.000\ttest accuracy=0.812\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "for epoch in range(50):\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for step, (batch_image, batch_label) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        # if torch.cuda.is_available():\n",
    "        #     batch_image, batch_label = batch_image.cuda(), batch_label.cuda()\n",
    "        batch_output = model(batch_image)\n",
    "        batch_loss = loss_func(batch_output, batch_label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += batch_loss.item()\n",
    "\n",
    "        # train accuracy\n",
    "        _, train_predicted = torch.max(batch_output.data, 1)\n",
    "        train_acc += (train_predicted == batch_label).sum().item()\n",
    "\n",
    "    train_acc /= train_size\n",
    "    running_loss /= (step+1)\n",
    "\n",
    "    # ----------test----------\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    for test_image, test_label in test_loader:\n",
    "        test_output = model(test_image)\n",
    "        _, predicted = torch.max(test_output.data, 1)\n",
    "        test_acc += (predicted == test_label).sum().item()\n",
    "    test_acc /= test_size\n",
    "\n",
    "    print('epoch={:d}\\ttrain loss={:.6f}\\ttrain accuracy={:.3f}\\ttest accuracy={:.3f}'.format(\n",
    "        epoch, running_loss, train_acc, test_acc))\n",
    "\n",
    "    if test_acc >= best_accuracy:\n",
    "        save_path = './trained_cnn/'\n",
    "        if not os.path.exists(save_path):\n",
    "             os.makedirs(save_path)\n",
    "        torch.save(model.state_dict(), './trained_cnn/FCNN_model.pkl')\n",
    "        best_accuracy = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b5e405934fd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
