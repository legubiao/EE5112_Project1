{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18133df0-507b-4eef-95cd-5db6122e91ab",
   "metadata": {},
   "source": [
    "# A guide for building a simple classifier based on the neural network to achieve binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fffaf6-ac01-44f6-a49b-f133d570c521",
   "metadata": {},
   "source": [
    "import the necessary python packages used in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a57db56-7926-421d-8f72-53ea044afeed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T16:52:10.774707Z",
     "start_time": "2023-09-07T16:52:09.328387Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27758e4b-652b-402b-b3e8-29bdeb02a360",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Generate our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e0995-48bc-45d2-ad71-d50a285074ea",
   "metadata": {},
   "source": [
    "Here, we randomly generate two sets of data (x1, x2), which are points in two-dimentional space. And then, labels are given to the two sets of data. For dataset x1, the label is 0. the label of dataset x2 is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56b547a-583b-46ce-ae64-59a8d4648682",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.randn(1000, 2)+5\n",
    "x2 = np.random.randn(1000, 2)+7\n",
    "y1 = np.zeros(x1.shape[0], dtype=int)\n",
    "y2 = np.ones(x2.shape[0], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32373c85-3f7f-427e-9345-8732600916ea",
   "metadata": {},
   "source": [
    "Here, both datasets x1 and x2 both contain 1000 data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86593264-ee48-444b-8fc4-a0ab3e103638",
   "metadata": {},
   "source": [
    "Notice: The size of dataset is very important for the performance of the neural network-based model. You should be careful about it when doing your own work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1438564-a69e-4946-b004-ddb8602b4c1e",
   "metadata": {},
   "source": [
    "To build a complete dataset for a classifier model, we need put the two datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c52d1f-c86e-43e1-8c41-99316c1251c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([x1, x2])  # input\n",
    "Y = np.concatenate([y1, y2])  # output (target/label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63975bbc-c94d-47e8-9b0d-091dd71336a0",
   "metadata": {},
   "source": [
    "Now, we can look at the basic properties of the dataset, like the size of the dataset, the dimention of each data and the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99b88ae6-2891-41bd-b8f0-5ba7fdf7b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size, data_dim = X.shape\n",
    "num_classes = int(np.max(Y)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4a723-bbf1-4138-b1da-e62df871ec9f",
   "metadata": {},
   "source": [
    "## 2. Build our neural network-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a68091-216e-499c-8406-8047cefca4a7",
   "metadata": {},
   "source": [
    "First, we need to design the neural network architecture and define it in a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a576ad7-1ad3-40cd-9ba3-929437e522d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_layer_size, hidden_layer_size, num_classes):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_layer_size, hidden_layer_size),\n",
    "            nn.BatchNorm1d(hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(hidden_layer_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef21ed8-deeb-4d59-b93a-6f4152619def",
   "metadata": {},
   "source": [
    "The name of the class is 'MyModel'. You can name it whatever you want.\n",
    "The parameters of this class should be given in def __init__(self, *args). Here, the paramters are input_layer_size, hidden_layer_size, num_classes.\n",
    "The neural network built here is a fully connected neural network with two layers. They are defined in the part of def __init__(self, *args).\n",
    "The forward propagation process is achieved in the function def forward(self, x)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dce816-cf33-488f-a86b-7003be8689bd",
   "metadata": {},
   "source": [
    "Notice: The architecture of the neural network is another vital aspect for the performance of the neural network-based model. You are encouraged to design and build your own network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766aeb42-15c2-45b8-a159-889e79f9fa5b",
   "metadata": {},
   "source": [
    "Now, we need to instantiate our model and set a optimizer for parameter update involved in this model. A loss function is also defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d539b0-a1bd-4a67-80e5-5e4ba8f9aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(input_layer_size=data_dim, hidden_layer_size=4, num_classes=num_classes)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb58fd1-305d-4fb1-8d46-7ca280c30135",
   "metadata": {},
   "source": [
    "## 3. train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22312862-fcf4-4d36-9cfe-dae94de5c92d",
   "metadata": {},
   "source": [
    "Before inputing the dataset to our model, we need to convert the type of our data from array to tensor and encapsulate it using the 'TensorDataset'. Then divide the dataset into training set and test set at the ratio of 8:2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f55ca25-6c2b-4690-8c06-3fa2ffdc66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils_data.TensorDataset(torch.Tensor(X), torch.LongTensor(Y))\n",
    "train_size = int(data_size * 0.8)\n",
    "test_size = data_size - train_size\n",
    "train_set, test_set = utils_data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = utils_data.DataLoader(dataset=train_set, batch_size=8, shuffle=True)\n",
    "test_loader = utils_data.DataLoader(dataset=test_set, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8e725-31c9-4fea-a7d6-5e07e90757e1",
   "metadata": {},
   "source": [
    "The following is the training and testing process in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b65344-6682-49ac-8423-8dc0301ac469",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0\n",
    "max_epoch = 100  # the maximum epoch for training\n",
    "for epoch in range(max_epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        batch_output = model(batch_x)\n",
    "        batch_loss = loss_func(batch_output, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()  # gradient is set to 0\n",
    "        batch_loss.backward()  # loss is back propagated\n",
    "        optimizer.step()       # update the model's parameters\n",
    "\n",
    "        # calculate loss\n",
    "        train_loss += batch_loss.item()\n",
    "        # calculate accuracy\n",
    "        _, batch_predicted = torch.max(batch_output, dim=1)\n",
    "        train_acc += (batch_predicted == batch_y).sum().item()\n",
    "\n",
    "    train_loss /= (step+1)\n",
    "    train_acc /= train_size\n",
    "\n",
    "    # test accuracy\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    for test_x, test_y in test_loader:\n",
    "        test_output = model(test_x)\n",
    "        _, test_predicted = torch.max(test_output, dim=1)\n",
    "        test_acc += (test_predicted == test_y).sum().item()\n",
    "    test_acc /= test_size\n",
    "\n",
    "    print('epoch={:d}\\ttrain loss={:.6f}\\ttrain accuracy={:.3f}\\ttest accuracy={:.3f}'.format(\n",
    "        epoch, train_loss, train_acc, test_acc))\n",
    "\n",
    "    if test_acc >= best_accuracy:\n",
    "        torch.save(model.state_dict(), 'classifier.pkl')  # the current parameters of the model are saved in the file 'classifier.pkl'\n",
    "        best_accuracy = test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f141ac67-5449-4663-b06d-c3e6b8dab356",
   "metadata": {},
   "source": [
    "Notice: The maximum epoch for training is an adjustable parameter, which also has an effect on the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9a8e9-e00d-4c04-9972-d7ea64ea8414",
   "metadata": {},
   "source": [
    "Here, we give a visualization display for the two dataset. The red and green points respectively represent the class 0 and class 1. The training data and test data are shown as solid points and squre points. The prediction results are displied using triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c128fa2-6044-46f9-8921-690dfcc571d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for train_x, train_y in train_loader:\n",
    "    train_x = np.array(train_x.data)\n",
    "    train_y = np.array(train_y.data)\n",
    "    index_class_0 = train_y == 0\n",
    "    index_class_1 = train_y == 1\n",
    "\n",
    "    p1, = plt.plot(train_x[index_class_0, 0], train_x[index_class_0, 1], 'ro', alpha=0.3)\n",
    "    p2, = plt.plot(train_x[index_class_1, 0], train_x[index_class_1, 1], 'go', alpha=0.3)\n",
    "\n",
    "for test_x, test_y in test_loader:\n",
    "    test_output = model(test_x)\n",
    "\n",
    "    test_x = np.array(test_x.data)\n",
    "    test_y = np.array(test_y.data)\n",
    "    index_class_0 = test_y == 0\n",
    "    index_class_1 = test_y == 1\n",
    "    p3, = plt.plot(test_x[index_class_0, 0], test_x[index_class_0, 1], 'rs')\n",
    "    p4, = plt.plot(test_x[index_class_1, 0], test_x[index_class_1, 1], 'gs')\n",
    "\n",
    "    _, test_predicted = torch.max(test_output, dim=1)\n",
    "    test_predicted = np.array(test_predicted.data)\n",
    "    pred_class_0 = test_predicted == 0\n",
    "    pred_class_1 = test_predicted == 1\n",
    "    p5, = plt.plot(test_x[pred_class_0, 0], test_x[pred_class_0, 1], 'r^')\n",
    "    p6, = plt.plot(test_x[pred_class_1, 0], test_x[pred_class_1, 1], 'g^')\n",
    "\n",
    "plt.legend([p1, p2, p3, p4, p5, p6], ['class 0: train data', 'class 1: train data',\n",
    "                                      'class 0: test data', 'class 1: test data',\n",
    "                                      'class 0: prediction', 'class 1: prediction'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
